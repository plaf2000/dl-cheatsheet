% Purple color scheme
\setsectioncolors{130,90,160}{150,110,175}{248,242,255}{220,205,240}
\sectiontitle{Convolutional Networks}

\subsectiontitle{Convolutions}

\begin{subsubsectionbox}{Convolution definition}
$(f * g)(u) := \int_{-\infty}^{\infty} g(u-t)f(t)dt$
\end{subsubsectionbox}


\begin{subsubsectionbox}{Cross-correlation}
$(f \star g)(u) := \int_{-\infty}^{\infty} g(u+t)f(t)dt$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Fourier property}
$\mathcal{F}(f * g) = \mathcal{F}(f) \cdot \mathcal{F}(g)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Discrete convolution}
1D: $(f * g)[u] := \sum_{t=-\infty}^{\infty} f[t]g[u - t]$\\
2D: $(f * g)[u, v] := \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f[u - m, v - n]g[m, n]$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Discrete cross-correlation}
$(g \star f)[u] := \sum_{t=-\infty}^{\infty} g[t]f[u + t]$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Toeplitz matrices}
$(f * g) = \text{Toeplitz-Matrix}(g)f$\\
$G = \text{Toeplitz-Matrix}(g)$:\\
$\begin{bmatrix}
g[0] & g[-1] & g[-2] & \cdots & g[-(n-1)] \\
g[1] & g[0] & g[-1] & \ddots & g[-(n-2)]\\
g[2] & g[1] & g[0] & \ddots & \vdots\\
\vdots & \ddots & \ddots & \ddots & g[-1] \\
g[n-1] & g[n-2] & \cdots & g[1] & g[0] \\
\end{bmatrix}$ \\
Then $G_{ij} = g[i-j]$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Properties of Convolutions}
$(f * g)(u) = (g * f)(u) \implies *$ is commutative\\
$(f \star g)(u) = (g \star f)(-u) \implies \star$ is not\\
$(f \star g)(y) = (\mathcal{T}(f) * g)(y)$ with $(\mathcal{T}(f))(u) = f(-u)$\\
$\mathcal{S}_t(f * g)(u) = (\mathcal{S}_t(f) * g)(u) = (f * \mathcal{S}_t(g))(u)$ with $\mathcal{S}_t(f)(u) = f(u - t)$ $\implies$ * is equivariant under $\mathcal{S}_t$\\
$\mathcal{A}$ is a continuos linear operator. Then
\begin{nscenter}
$\mathcal{A}$ equivariant under translations $\iff$ $\mathcal{A}$ is a convolution: $\exists g$ s.t. $\mathcal{A}(f) = f * g$
\end{nscenter}
\end{subsubsectionbox}


\subsectiontitle{Convolutional Networks}

\begin{subsubsectionbox}{Conventions}
Padding: Add zeros around input. Stride: Step size of convolution.
\end{subsubsectionbox}

\begin{subsubsectionbox}{Max-Pooling}
Take maximum value in windows (size $r$)
\end{subsubsectionbox}

\begin{subsubsectionbox}{ConvNets for Images}
$y[r][s, t] = \sum_u \sum_{\Delta s,\Delta t} w[r, u][\Delta s, \Delta t] \cdot x[u][s + \Delta s, t + \Delta t]$ ($r$: output channel, $u$: input channel)
\end{subsubsectionbox}

\begin{subsubsectionbox}{Parameters count}
$D = \underbrace{\#r \cdot \#u}_{\text{channels}} \cdot \underbrace{\#\Delta s \cdot \#\Delta t}_{\text{window size}}$ (in-channels fully connected to out-channels)
\end{subsubsectionbox}

\begin{subsubsectionbox}{Separable Kernels}
    If $w=uv^\top \in \R^{q \times q}$ and $x \in \R^{d\times d}$, then $x * w$ can be computed in $\mathcal{O}(d(d-q)q)$ instead of $\mathcal{O}((d-q)^2q^2)$.
    
\end{subsubsectionbox}



\subsectiontitle{NLP with ConvNets}

\begin{subsubsectionbox}{Word embedding}
$\omega \mapsto x_\omega \in \R^n$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Conditional log-bilinear model}
$P(\nu | \omega) = \frac{\exp[x_\omega^\top y_\nu]}{\sum_\mu \exp[x_\omega^\top y_\mu]}$

$h(\{x_\omega\}, \{y_\nu\}) = \sum_{(\omega,\nu)} \ell_{\omega,\nu}$, $\ell_{\omega,\nu} = -x_\omega^\top y_\nu + \ln\sum_\mu \exp[x_\omega^\top y_\mu]$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Negative sampling}
$\tilde{\ell}_{\omega,\nu} = -\ln \sigma(x_\omega^\top y_\nu) - \beta \E_{\mu \sim D} \ln(1 - \sigma(x_\omega^\top y_\mu))$
\end{subsubsectionbox}

