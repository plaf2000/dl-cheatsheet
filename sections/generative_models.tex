% Olive color scheme
\setsectioncolors{120,130,50}{145,155,75}{250,252,235}{225,230,185}
\sectiontitle{Generative Models}

\subsectiontitle{Variational Auto Encoders}

\begin{subsubsectionbox}{Linear autoencoder}
$x \mapsto z = Cx$, $z \mapsto \hat{x} = Dz$, $E(C,D)(x) = \frac{1}{2}\|x - DCx\|^2$

$DCX = \hat{X} = U\Sigma_m V^\top$, for centered data $\equiv$ PCA
\end{subsubsectionbox}

\begin{subsubsectionbox}{Linear factor analysis}
$x = \mu + Wz + \eta$, $\eta \sim \N(0, \Psi)$, $x \sim \N(\mu, WW^\top + \Psi)$ for $z \sim \N(0,I)$

$\mu_{z|x} = W^\top(WW^\top + \Psi)^{-1}(x - \mu)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Generative model}
$p_\theta(x,z) = p_\theta(x|z)p(z)$, $p(z) = \N(0,I)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{ELBO}
$\log p(\theta)(x) = \log \int q(z)p(\theta)(x|z)\frac{p(z)}{q(z)}dz$

$\geq \int q(z)\log p(\theta)(x|z)dz - D_{KL}(q\|p) =: L(\theta, q)(x)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Inference network}
$z \sim \N(\mu(x), \Sigma(x))$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Encoder}
$q_\phi(z|x) = \N(\mu_\phi(x), \sigma_\phi^2(x))$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Decoder}
$p_\theta(x|z) = \N(\mu_\theta(z), \sigma^2 I)$ or Bernoulli
\end{subsubsectionbox}

\begin{subsubsectionbox}{Reparameterization trick}
$z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon$, $\epsilon \sim \N(0,I)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{KL divergence (Gaussian)}
$D_{KL} = \frac{1}{2}\sum_j (\mu_j^2 + \sigma_j^2 - \ln\sigma_j^2 - 1)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{$\beta$-VAE}
$\cL = \E_q[\ln p(x|z)] - \beta D_{KL}(q \| p)$. $\beta > 1$ for disentanglement.
\end{subsubsectionbox}

\subsectiontitle{Generative Adversarial Networks}

\begin{subsubsectionbox}{Generator}
$G: z \mapsto x$, $z \sim p(z)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Discriminator}
$D: x \mapsto [0,1]$, probability that $x$ is real
\end{subsubsectionbox}

\begin{subsubsectionbox}{GAN objective}
$V(G,D) = \E_{x_r \sim p_{\text{data}}}[D(x_r)] + \E_{z \sim p_z}[1 - D(G(z))]$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Bayes-optimal classifier}
$q_\theta(x) := P\{y=1|x\} = \frac{p(x)}{p(x) + p_\theta(x)}$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Jensen-Shannon objective}
$\ell^* = \text{JS}(p, p_\theta) - \ln 2$

$\ell^*(\theta) \geq \sup_{\phi} \ell(\theta, \phi)$ where $\phi$: discriminator, $\theta$: generator
\end{subsubsectionbox}

\begin{subsubsectionbox}{Alternating gradient descent}
$\theta_{t+1} = \theta_t - \eta \nabla_\theta \ell(\theta_t, \phi_t)$

$\phi_{t+1} = \phi_t + \eta \nabla_\phi \ell(\theta^{t+1}, \phi_t)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Wasserstein GAN}
$\min_G \max_{D \in 1\text{-Lip}} \E_x[D(x)] - \E_z[D(G(z))]$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Gradient penalty (WGAN-GP)}
$\lambda \E_{\hat{x}}[(\|\nabla_{\hat{x}} D(\hat{x})\|_2 - 1)^2]$, $\hat{x} = \alpha x + (1-\alpha)G(z)$
\end{subsubsectionbox}

\subsectiontitle{Denoising Diffusion}

\begin{subsubsectionbox}{Forward process}
$q(x_t|x_{t-1}) = \N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Marginal}
$q(x_t|x_0) = \N(\sqrt{\bar{\alpha}_t}x_0, \bar{\beta}_t I)$, $\bar{\alpha}_t = \prod_{\tau=1}^t (1-\beta_\tau)$, $\bar{\beta}_t = 1 - \bar{\alpha}_t$

$\nu_t \approx \N(\sqrt{\bar{\alpha}_t}x_0, \bar{\beta}_t I) \xrightarrow{t\to\infty} \N(0,I)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Reverse process}
$p_\theta(x_{t-1}|x_t) = \N(m_\theta(x_t, t), \Sigma(x_t, t))$

Forward: $\pi^* \to \nu_T = \pi$. Backward: $\pi \to \mu_0^\theta \approx \pi^*$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Training objective}
$L_t = \frac{\|m(x_t, x_0, t) - m_\theta(x_t, t)\|^2}{2\sigma_t^2} + \text{const}$

Reparameterization: $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Simplified criterion}
$h(\theta)(x) = \frac{1}{T}\sum_{t=1}^T \E[\|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x + \sqrt{1-\bar{\alpha}_t}\epsilon, t)\|^2]$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Forward trajectory target}
$x_{t-1}|x_t, x_0 = \N(m(x_t, x_0, t), \tilde{\beta}_t I)$

$m(x_t, x_0, t) = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 + \frac{(1-\bar{\alpha}_{t-1})\sqrt{1-\beta_t}}{1-\bar{\alpha}_t}x_t$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Sampling}
$m(x_t, x_0, t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Score function}
$\nabla_x \ln p(x) \approx -\frac{\epsilon_\theta(x,t)}{\sqrt{1-\bar{\alpha}_t}}$
\end{subsubsectionbox}

\begin{subsubsectionbox}{Classifier-free guidance}
$\tilde{\epsilon} = (1+w)\epsilon_\theta(x_t,t,c) - w\epsilon_\theta(x_t,t)$
\end{subsubsectionbox}

\begin{subsubsectionbox}{DDIM (deterministic)}
$x_{t-1} = \sqrt{\bar{\alpha}_{t-1}}(\frac{x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_\theta}{\sqrt{\bar{\alpha}_t}}) + \sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta$
\end{subsubsectionbox}

